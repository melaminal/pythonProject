# Back propagation - алгоритм обучения по методу обратного распространения [обучение с учителем]
# (https://www.youtube.com/watch?v=UXB9bFj-UA4&list=PLA0M1Bcd0w8yv0XGiF1wjerjSZVSrYbjh&index=4)

# d - желаемый отклик
# y = f(v_out): выходное значение
# e = y - d : ошибка
# f(x) - функция активации нейрона (для сетей с небольшим кол-вом нейронов исп-ся гиперболический тангенс либо логистическая функция)
# delta = e * f'(v_out) = e * y * (1-y): локальный градиент для выходного нейрона
# lambda - шаг сходимости (0.1, 0.01, 0.001 и т.д.)

import numpy as np

# гиперболический тангенс
def f(x):
    return 2/(1 + np.exp(-x)) - 1

# производная
def df(x):
    return 0.5*(1 + x)*(1 - x)

# случайные веса для каждой связи
W1 = np.array([[-0.2, 0.3, -0.4], [0.1, -0.3, -0.4]])
W2 = np.array([0.2, 0.3])

# ф-ия, пропускающая вектор наблюдений через нейронную сеть
def do_forward(inp):
    sum = np.dot(W1, inp)
    out = np.array([f(x) for x in sum])

    sum  = np.dot(W2, out)
    y = f(sum)
    return (y, out) # для каждого нейрона запоминаются выходные значения

# ф-ия, обучающая сеть
def train(epoch):
    global W1, W2
    lmd = 0.01 # шаг обучения
    N = 10000  # число итераций при обучении
    count = len(epoch)
    for k in range(N):
        x = epoch[np.random.randint(0, count)] # случайный выбор входного сигнала из обучающей выборки
        y, out = do_forward(x[0:3])                 # прямой проход по НС и вычисление выходных значений нейронов
        e = y - x[-1]                               # ошибка
        delta = e * df(y)                           # локальный градиент
        W2[0] = W2[0] - lmd * delta * out[0]        # корректировка веса первой связи последнего слоя
        W2[1] = W2[1] - lmd * delta * out[1]        # корректировка веса второй связи последнего слоя

        delta2 = W2 * delta * df(out)               # вектор из 2-х величин локальных градиентов для нейронов скрытого слоя

        # корректировка 3х входящих связей первого слоя
        W1[0, :] = W1[0, :] - np.array(x[0:3]) * delta2[0] * lmd
        W1[1, :] = W1[1, :] - np.array(x[0:3]) * delta2[1] * lmd

# обучающая выборка (она же полная выборка): 3 входных вектора (1 или -1) + выходное значение (1 или -1)
epoch = [(-1, -1, -1, -1),
         (-1, -1, 1, 1),
         (-1, 1, -1, -1),
         (-1, 1, 1, 1),
         (1, -1, -1, -1),
         (1, -1, 1, 1),
         (1, 1, -1, -1),
         (1, 1, 1, -1)]

train(epoch) # запуск обучения сети

# проверка полученных результатов
for x in epoch:
    y, out = do_forward(x[0:3])
    print(f"Выходное значение НС: {y} => {x[-1]}")

# близко к 1 == 1, близко к 0 или -1 == -1